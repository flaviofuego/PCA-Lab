{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification, make_blobs\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 10)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"ANÁLISIS COMPLETO DE PCA - IMPLEMENTACIÓN DESDE CERO\")\n",
        "print(\"=\" * 70)\n",
        "print(\"Librerías cargadas exitosamente\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Matrix:\n",
        "    def __init__(self, rows, cols):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.data = np.zeros((rows, cols))\n",
        "    \n",
        "    def __getitem__(self, key):\n",
        "        return self.data[key]\n",
        "    \n",
        "    def __setitem__(self, key, value):\n",
        "        self.data[key] = value\n",
        "    \n",
        "    def copy(self):\n",
        "        new_matrix = Matrix(self.rows, self.cols)\n",
        "        new_matrix.data = self.data.copy()\n",
        "        return new_matrix\n",
        "    \n",
        "    def transpose(self):\n",
        "        new_matrix = Matrix(self.cols, self.rows)\n",
        "        new_matrix.data = self.data.T\n",
        "        return new_matrix\n",
        "    \n",
        "    def multiply(self, other):\n",
        "        if self.cols != other.rows:\n",
        "            raise ValueError(\"Dimensiones incompatibles para multiplicación\")\n",
        "        result = Matrix(self.rows, other.cols)\n",
        "        result.data = np.dot(self.data, other.data)\n",
        "        return result\n",
        "    \n",
        "    def print_matrix(self, name=\"Matrix\", max_rows=5, max_cols=5):\n",
        "        print(f\"\\n{name} ({self.rows} x {self.cols}):\")\n",
        "        rows_to_show = min(max_rows, self.rows)\n",
        "        cols_to_show = min(max_cols, self.cols)\n",
        "        \n",
        "        for i in range(rows_to_show):\n",
        "            for j in range(cols_to_show):\n",
        "                print(f\"{self.data[i, j]:10.6f}\", end=\" \")\n",
        "            if self.cols > cols_to_show:\n",
        "                print(\"...\", end=\"\")\n",
        "            print()\n",
        "        if self.rows > rows_to_show:\n",
        "            print(\"...\")\n",
        "        print()\n",
        "\n",
        "print(\"✓ Clase Matrix implementada\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_synthetic_data(n_samples=500, n_features=10, n_informative=8, \n",
        "                           n_redundant=2, random_state=42):\n",
        "    print(f\"Generando datos sintéticos...\")\n",
        "    print(f\"  - Muestras (N): {n_samples}\")\n",
        "    print(f\"  - Dimensiones (M): {n_features}\")\n",
        "    print(f\"  - Características informativas: {n_informative}\")\n",
        "    print(f\"  - Características redundantes: {n_redundant}\")\n",
        "    \n",
        "    X, y = make_classification(\n",
        "        n_samples=n_samples,\n",
        "        n_features=n_features,\n",
        "        n_informative=n_informative,\n",
        "        n_redundant=n_redundant,\n",
        "        n_clusters_per_class=2,\n",
        "        random_state=random_state,\n",
        "        shuffle=True\n",
        "    )\n",
        "    \n",
        "    X = X * np.random.uniform(0.5, 2.0, size=n_features)\n",
        "    X = X + np.random.uniform(-5, 5, size=n_features)\n",
        "    \n",
        "    print(f\"  - Shape de los datos: {X.shape}\")\n",
        "    print(f\"  - Media de cada dimensión: {X.mean(axis=0)[:3]}... (primeras 3)\")\n",
        "    print(f\"  - Desviación estándar: {X.std(axis=0)[:3]}... (primeras 3)\")\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "def generate_blobs_data(n_samples=500, n_features=10, centers=3, random_state=42):\n",
        "    print(f\"Generando datos de clusters...\")\n",
        "    print(f\"  - Muestras (N): {n_samples}\")\n",
        "    print(f\"  - Dimensiones (M): {n_features}\")\n",
        "    print(f\"  - Número de clusters: {centers}\")\n",
        "    \n",
        "    X, y = make_blobs(\n",
        "        n_samples=n_samples,\n",
        "        n_features=n_features,\n",
        "        centers=centers,\n",
        "        cluster_std=2.0,\n",
        "        random_state=random_state\n",
        "    )\n",
        "    \n",
        "    print(f\"  - Shape de los datos: {X.shape}\")\n",
        "    return X, y\n",
        "\n",
        "print(\"✓ Funciones de generación de datos implementadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_mean(matrix):\n",
        "    if isinstance(matrix, Matrix):\n",
        "        data = matrix.data\n",
        "    else:\n",
        "        data = matrix\n",
        "    \n",
        "    mean = np.mean(data, axis=0)\n",
        "    return mean\n",
        "\n",
        "def center_data(matrix, mean):\n",
        "    if isinstance(matrix, Matrix):\n",
        "        matrix.data = matrix.data - mean\n",
        "    else:\n",
        "        matrix = matrix - mean\n",
        "    return matrix\n",
        "\n",
        "def compute_covariance(matrix):\n",
        "    if isinstance(matrix, Matrix):\n",
        "        data = matrix.data\n",
        "    else:\n",
        "        data = matrix\n",
        "    \n",
        "    n = data.shape[0]\n",
        "    cov = np.dot(data.T, data) / (n - 1)\n",
        "    return cov\n",
        "\n",
        "def vector_norm(vec):\n",
        "    return np.sqrt(np.sum(vec**2))\n",
        "\n",
        "def vector_normalize(vec):\n",
        "    norm = vector_norm(vec)\n",
        "    if norm > 1e-10:\n",
        "        return vec / norm\n",
        "    return vec\n",
        "\n",
        "def vector_dot(vec1, vec2):\n",
        "    return np.dot(vec1, vec2)\n",
        "\n",
        "print(\"✓ Funciones estadísticas básicas implementadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_eigen(cov_matrix, max_iterations=1000, tolerance=1e-10):\n",
        "    n = cov_matrix.shape[0]\n",
        "    A = cov_matrix.copy()\n",
        "    eigenvalues = np.zeros(n)\n",
        "    eigenvectors = np.zeros((n, n))\n",
        "    \n",
        "    for k in range(n):\n",
        "        v = np.ones(n) / np.sqrt(n)\n",
        "        lambda_val = 0.0\n",
        "        \n",
        "        for iter in range(max_iterations):\n",
        "            v_new = np.dot(A, v)\n",
        "            lambda_new = np.dot(v_new, v)\n",
        "            v_new = vector_normalize(v_new)\n",
        "            \n",
        "            if abs(lambda_new - lambda_val) < tolerance:\n",
        "                lambda_val = lambda_new\n",
        "                v = v_new\n",
        "                break\n",
        "            \n",
        "            lambda_val = lambda_new\n",
        "            v = v_new\n",
        "        \n",
        "        eigenvalues[k] = lambda_val\n",
        "        eigenvectors[:, k] = v\n",
        "        \n",
        "        A = A - lambda_val * np.outer(v, v)\n",
        "    \n",
        "    return eigenvalues, eigenvectors\n",
        "\n",
        "def sort_eigen(eigenvalues, eigenvectors):\n",
        "    n = len(eigenvalues)\n",
        "    for i in range(n - 1):\n",
        "        for j in range(n - i - 1):\n",
        "            if eigenvalues[j] < eigenvalues[j + 1]:\n",
        "                eigenvalues[j], eigenvalues[j + 1] = eigenvalues[j + 1], eigenvalues[j]\n",
        "                eigenvectors[:, j], eigenvectors[:, j + 1] = eigenvectors[:, j + 1], eigenvectors[:, j].copy()\n",
        "\n",
        "print(\"✓ Algoritmo de eigenvalores implementado\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PCAModel:\n",
        "    def __init__(self, n_components):\n",
        "        self.n_components = n_components\n",
        "        self.mean = None\n",
        "        self.eigenvalues = None\n",
        "        self.eigenvectors = None\n",
        "        self.explained_variance_ratio = 0.0\n",
        "\n",
        "def pca_fit(data, n_components):\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"ENTRENANDO MODELO PCA\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Input shape: {data.shape[0]} muestras x {data.shape[1]} características\")\n",
        "    print(f\"Componentes objetivo: {n_components}\")\n",
        "    \n",
        "    model = PCAModel(n_components)\n",
        "    \n",
        "    mean = compute_mean(data)\n",
        "    model.mean = mean\n",
        "    \n",
        "    data_centered = center_data(data.copy(), mean)\n",
        "    \n",
        "    cov_matrix = compute_covariance(data_centered)\n",
        "    print(f\"Covariance matrix: {cov_matrix.shape[0]} x {cov_matrix.shape[1]}\")\n",
        "    \n",
        "    eigenvalues, eigenvectors = compute_eigen(cov_matrix)\n",
        "    sort_eigen(eigenvalues, eigenvectors)\n",
        "    \n",
        "    model.eigenvalues = eigenvalues\n",
        "    model.eigenvectors = eigenvectors\n",
        "    \n",
        "    total_variance = np.sum(eigenvalues)\n",
        "    explained_variance = np.sum(eigenvalues[:n_components])\n",
        "    model.explained_variance_ratio = explained_variance / total_variance\n",
        "    \n",
        "    print(f\"\\nVarianza explicada: {model.explained_variance_ratio:.4f} ({model.explained_variance_ratio*100:.2f}%)\")\n",
        "    print(f\"\\nTop eigenvalues:\")\n",
        "    for i in range(min(n_components, 5)):\n",
        "        print(f\"  PC{i+1}: {eigenvalues[i]:.6f}\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "def pca_transform(model, data):\n",
        "    data_centered = center_data(data.copy(), model.mean)\n",
        "    components = model.eigenvectors[:, :model.n_components]\n",
        "    projected = np.dot(data_centered, components)\n",
        "    return projected\n",
        "\n",
        "print(\"✓ Clase PCAModel y funciones PCA implementadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PASO 1: GENERACIÓN DE DATOS SINTÉTICOS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "n_samples = 500\n",
        "n_features = 10\n",
        "n_components = 2\n",
        "\n",
        "X, y = generate_synthetic_data(\n",
        "    n_samples=n_samples,\n",
        "    n_features=n_features,\n",
        "    n_informative=max(2, n_features - 2),\n",
        "    n_redundant=min(2, n_features // 2),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Datos generados exitosamente\")\n",
        "print(f\"  - Shape: {X.shape}\")\n",
        "print(f\"  - Clases únicas: {len(np.unique(y))}\")\n",
        "print(f\"  - Distribución de clases: {np.bincount(y)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PASO 2: APLICACIÓN DE PCA - IMPLEMENTACIÓN PROPIA\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "pca_model = pca_fit(X, n_components)\n",
        "X_pca_custom = pca_transform(pca_model, X)\n",
        "\n",
        "print(f\"\\n✓ PCA aplicado exitosamente\")\n",
        "print(f\"  - Shape original: {X.shape}\")\n",
        "print(f\"  - Shape reducido: {X_pca_custom.shape}\")\n",
        "print(f\"  - Reducción dimensional: {(1 - n_components/n_features)*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PASO 3: APLICACIÓN DE PCA - SKLEARN (REFERENCIA)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "pca_sklearn = PCA(n_components=n_components)\n",
        "X_pca_sklearn = pca_sklearn.fit_transform(X)\n",
        "\n",
        "print(f\"✓ PCA sklearn aplicado\")\n",
        "print(f\"  - Shape: {X_pca_sklearn.shape}\")\n",
        "print(f\"  - Varianza explicada: {pca_sklearn.explained_variance_ratio_.sum():.4f} ({pca_sklearn.explained_variance_ratio_.sum()*100:.2f}%)\")\n",
        "print(f\"  - Componentes principales:\")\n",
        "for i, var in enumerate(pca_sklearn.explained_variance_ratio_):\n",
        "    print(f\"    PC{i+1}: {var*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PASO 4: COMPARACIÓN NUMÉRICA\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "X_pca_custom_adjusted = X_pca_custom.copy()\n",
        "flipped_components = []\n",
        "\n",
        "for i in range(X_pca_custom.shape[1]):\n",
        "    corr = np.corrcoef(X_pca_sklearn[:, i], X_pca_custom[:, i])[0, 1]\n",
        "    if corr < 0:\n",
        "        X_pca_custom_adjusted[:, i] = -X_pca_custom[:, i]\n",
        "        flipped_components.append(i + 1)\n",
        "\n",
        "mse = mean_squared_error(X_pca_sklearn, X_pca_custom_adjusted)\n",
        "mae = mean_absolute_error(X_pca_sklearn, X_pca_custom_adjusted)\n",
        "\n",
        "correlations = []\n",
        "for i in range(X_pca_custom.shape[1]):\n",
        "    corr = np.corrcoef(X_pca_sklearn[:, i], X_pca_custom_adjusted[:, i])[0, 1]\n",
        "    correlations.append(corr)\n",
        "\n",
        "diff = X_pca_sklearn - X_pca_custom_adjusted\n",
        "max_diff = np.max(np.abs(diff))\n",
        "mean_diff = np.mean(np.abs(diff))\n",
        "\n",
        "sign_note = f\" (componentes invertidos: PC{', PC'.join(map(str, flipped_components))})\" if flipped_components else \"\"\n",
        "\n",
        "print(f\"\\nMétricas de comparación{sign_note}:\")\n",
        "print(f\"  - MSE (Error Cuadrático Medio):        {mse:.6e}\")\n",
        "print(f\"  - MAE (Error Absoluto Medio):          {mae:.6e}\")\n",
        "print(f\"  - Diferencia máxima:                   {max_diff:.6e}\")\n",
        "print(f\"  - Diferencia media (abs):              {mean_diff:.6e}\")\n",
        "print(f\"\\nCorrelación por componente:\")\n",
        "for i, corr in enumerate(correlations):\n",
        "    print(f\"  - PC{i+1}: {corr:.6f}\")\n",
        "\n",
        "avg_corr = np.mean(correlations)\n",
        "if avg_corr > 0.99:\n",
        "    status = \"EXCELENTE ✓\"\n",
        "elif avg_corr > 0.95:\n",
        "    status = \"BUENA ✓\"\n",
        "else:\n",
        "    status = \"REVISAR ⚠\"\n",
        "\n",
        "print(f\"\\nEstado de la implementación: {status}\")\n",
        "print(f\"Correlación promedio: {avg_corr:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PASO 5: VISUALIZACIONES COMPARATIVAS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "colors = plt.cm.Set1(np.linspace(0, 1, len(np.unique(y))))\n",
        "\n",
        "for idx, class_label in enumerate(np.unique(y)):\n",
        "    mask = y == class_label\n",
        "    axes[0, 0].scatter(X_pca_custom_adjusted[mask, 0], X_pca_custom_adjusted[mask, 1], \n",
        "                       alpha=0.6, s=30, label=f'Clase {int(class_label)}', c=[colors[idx]])\n",
        "axes[0, 0].set_xlabel('PC1')\n",
        "axes[0, 0].set_ylabel('PC2')\n",
        "axes[0, 0].set_title('PCA - Implementación Propia')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "for idx, class_label in enumerate(np.unique(y)):\n",
        "    mask = y == class_label\n",
        "    axes[0, 1].scatter(X_pca_sklearn[mask, 0], X_pca_sklearn[mask, 1], \n",
        "                       alpha=0.6, s=30, label=f'Clase {int(class_label)}', c=[colors[idx]])\n",
        "axes[0, 1].set_xlabel('PC1')\n",
        "axes[0, 1].set_ylabel('PC2')\n",
        "axes[0, 1].set_title('PCA - sklearn')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 0].scatter(X_pca_sklearn[:, 0], X_pca_custom_adjusted[:, 0], alpha=0.5, s=20)\n",
        "min_val = min(X_pca_sklearn[:, 0].min(), X_pca_custom_adjusted[:, 0].min())\n",
        "max_val = max(X_pca_sklearn[:, 0].max(), X_pca_custom_adjusted[:, 0].max())\n",
        "axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect match')\n",
        "axes[1, 0].set_xlabel('sklearn PC1')\n",
        "axes[1, 0].set_ylabel('Custom PC1')\n",
        "axes[1, 0].set_title('Correlación PC1')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "axes[1, 0].set_aspect('equal', adjustable='box')\n",
        "\n",
        "axes[1, 1].scatter(X_pca_sklearn[:, 1], X_pca_custom_adjusted[:, 1], alpha=0.5, s=20)\n",
        "min_val = min(X_pca_sklearn[:, 1].min(), X_pca_custom_adjusted[:, 1].min())\n",
        "max_val = max(X_pca_sklearn[:, 1].max(), X_pca_custom_adjusted[:, 1].max())\n",
        "axes[1, 1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect match')\n",
        "axes[1, 1].set_xlabel('sklearn PC2')\n",
        "axes[1, 1].set_ylabel('Custom PC2')\n",
        "axes[1, 1].set_title('Correlación PC2')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "axes[1, 1].set_aspect('equal', adjustable='box')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Visualizaciones generadas exitosamente\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PASO 6: ANÁLISIS DE CONTORNOS Y SUPERPOSICIÓN\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 12))\n",
        "\n",
        "try:\n",
        "    from scipy.stats import gaussian_kde\n",
        "    \n",
        "    for idx, class_label in enumerate(np.unique(y)):\n",
        "        mask = y == class_label\n",
        "        \n",
        "        if np.sum(mask) > 3:\n",
        "            xy_sklearn = np.vstack([X_pca_sklearn[mask, 0], X_pca_sklearn[mask, 1]])\n",
        "            kde_sklearn = gaussian_kde(xy_sklearn)\n",
        "            \n",
        "            xy_custom = np.vstack([X_pca_custom_adjusted[mask, 0], X_pca_custom_adjusted[mask, 1]])\n",
        "            kde_custom = gaussian_kde(xy_custom)\n",
        "            \n",
        "            x_min = min(X_pca_sklearn[:, 0].min(), X_pca_custom_adjusted[:, 0].min()) - 2\n",
        "            x_max = max(X_pca_sklearn[:, 0].max(), X_pca_custom_adjusted[:, 0].max()) + 2\n",
        "            y_min = min(X_pca_sklearn[:, 1].min(), X_pca_custom_adjusted[:, 1].min()) - 2\n",
        "            y_max = max(X_pca_sklearn[:, 1].max(), X_pca_custom_adjusted[:, 1].max()) + 2\n",
        "            \n",
        "            xx, yy = np.mgrid[x_min:x_max:100j, y_min:y_max:100j]\n",
        "            positions = np.vstack([xx.ravel(), yy.ravel()])\n",
        "            \n",
        "            z_sklearn = np.reshape(kde_sklearn(positions).T, xx.shape)\n",
        "            z_custom = np.reshape(kde_custom(positions).T, xx.shape)\n",
        "            \n",
        "            ax.contour(xx, yy, z_sklearn, levels=5, colors=['blue'], alpha=0.6, linewidths=2, linestyles='solid')\n",
        "            ax.contour(xx, yy, z_custom, levels=5, colors=['red'], alpha=0.6, linewidths=2, linestyles='dashed')\n",
        "    \n",
        "    for idx, class_label in enumerate(np.unique(y)):\n",
        "        mask = y == class_label\n",
        "        ax.scatter(X_pca_sklearn[mask, 0], X_pca_sklearn[mask, 1],\n",
        "                  c=[colors[idx]], alpha=0.5, s=50,\n",
        "                  edgecolors='blue', linewidths=1.5,\n",
        "                  marker='o', label=f'sklearn Clase {int(class_label)}')\n",
        "        \n",
        "        ax.scatter(X_pca_custom_adjusted[mask, 0], X_pca_custom_adjusted[mask, 1],\n",
        "                  c=[colors[idx]], alpha=1.0, s=60,\n",
        "                  edgecolors='red', linewidths=2.0,\n",
        "                  marker='x', label=f'Custom Clase {int(class_label)}')\n",
        "    \n",
        "    ax.set_xlabel('PC1', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('PC2', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Superposición de Contornos\\nsklearn (azul sólido) | Custom (rojo punteado)', \n",
        "                fontsize=14, fontweight='bold', pad=20)\n",
        "    ax.grid(True, alpha=0.2, linestyle='--')\n",
        "    ax.legend(loc='best', framealpha=0.9, fontsize=8, ncol=2)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"✓ Análisis de contornos completado\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"⚠ scipy no disponible, saltando análisis de contornos\")\n",
        "    print(\"  Instalar con: pip install scipy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PASO 7: ANÁLISIS DE DISTRIBUCIONES DE DIFERENCIAS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "fig, axes = plt.subplots(1, n_components, figsize=(8*n_components, 6))\n",
        "\n",
        "if n_components == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for i in range(n_components):\n",
        "    diff = X_pca_sklearn[:, i] - X_pca_custom_adjusted[:, i]\n",
        "    \n",
        "    axes[i].hist(diff, bins=50, alpha=0.7, edgecolor='black')\n",
        "    axes[i].axvline(0, color='red', linestyle='--', linewidth=2, label='Cero')\n",
        "    axes[i].axvline(np.mean(diff), color='green', linestyle='-', \n",
        "                   linewidth=2, label=f'Media: {np.mean(diff):.2e}')\n",
        "    \n",
        "    axes[i].set_xlabel('Diferencia (sklearn - Custom)')\n",
        "    axes[i].set_ylabel('Frecuencia')\n",
        "    axes[i].set_title(f'Distribución de Diferencias - PC{i+1}')\n",
        "    axes[i].legend()\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Análisis de distribuciones completado\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PASO 8: ANÁLISIS CON DIFERENTES TIPOS DE DATOS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"Probando con datos de clusters...\")\n",
        "X_blobs, y_blobs = generate_blobs_data(\n",
        "    n_samples=300,\n",
        "    n_features=8,\n",
        "    centers=4,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "pca_model_blobs = pca_fit(X_blobs, 2)\n",
        "X_pca_blobs_custom = pca_transform(pca_model_blobs, X_blobs)\n",
        "\n",
        "pca_sklearn_blobs = PCA(n_components=2)\n",
        "X_pca_blobs_sklearn = pca_sklearn_blobs.fit_transform(X_blobs)\n",
        "\n",
        "X_pca_blobs_custom_adjusted = X_pca_blobs_custom.copy()\n",
        "for i in range(X_pca_blobs_custom.shape[1]):\n",
        "    corr = np.corrcoef(X_pca_blobs_sklearn[:, i], X_pca_blobs_custom[:, i])[0, 1]\n",
        "    if corr < 0:\n",
        "        X_pca_blobs_custom_adjusted[:, i] = -X_pca_blobs_custom[:, i]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "colors_blobs = plt.cm.Set1(np.linspace(0, 1, len(np.unique(y_blobs))))\n",
        "\n",
        "for idx, class_label in enumerate(np.unique(y_blobs)):\n",
        "    mask = y_blobs == class_label\n",
        "    axes[0].scatter(X_pca_blobs_custom_adjusted[mask, 0], X_pca_blobs_custom_adjusted[mask, 1], \n",
        "                   alpha=0.6, s=30, label=f'Cluster {int(class_label)}', c=[colors_blobs[idx]])\n",
        "axes[0].set_xlabel('PC1')\n",
        "axes[0].set_ylabel('PC2')\n",
        "axes[0].set_title('PCA Custom - Datos de Clusters')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "for idx, class_label in enumerate(np.unique(y_blobs)):\n",
        "    mask = y_blobs == class_label\n",
        "    axes[1].scatter(X_pca_blobs_sklearn[mask, 0], X_pca_blobs_sklearn[mask, 1], \n",
        "                   alpha=0.6, s=30, label=f'Cluster {int(class_label)}', c=[colors_blobs[idx]])\n",
        "axes[1].set_xlabel('PC1')\n",
        "axes[1].set_ylabel('PC2')\n",
        "axes[1].set_title('PCA sklearn - Datos de Clusters')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Análisis con datos de clusters completado\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PASO 9: ANÁLISIS DE VARIANZA EXPLICADA\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "eigenvalues_custom = pca_model.eigenvalues\n",
        "eigenvalues_sklearn = pca_sklearn.explained_variance_\n",
        "\n",
        "axes[0].bar(range(1, len(eigenvalues_custom) + 1), eigenvalues_custom, alpha=0.7, label='Custom', color='blue')\n",
        "axes[0].bar(range(1, len(eigenvalues_sklearn) + 1), eigenvalues_sklearn, alpha=0.7, label='sklearn', color='red')\n",
        "axes[0].set_xlabel('Componente Principal')\n",
        "axes[0].set_ylabel('Eigenvalue')\n",
        "axes[0].set_title('Comparación de Eigenvalues')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "explained_var_custom = eigenvalues_custom / np.sum(eigenvalues_custom)\n",
        "explained_var_sklearn = pca_sklearn.explained_variance_ratio_\n",
        "\n",
        "axes[1].bar(range(1, len(explained_var_custom) + 1), explained_var_custom, alpha=0.7, label='Custom', color='blue')\n",
        "axes[1].bar(range(1, len(explained_var_sklearn) + 1), explained_var_sklearn, alpha=0.7, label='sklearn', color='red')\n",
        "axes[1].set_xlabel('Componente Principal')\n",
        "axes[1].set_ylabel('Varianza Explicada')\n",
        "axes[1].set_title('Comparación de Varianza Explicada')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Análisis de varianza completado\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PASO 10: REPORTE FINAL Y CONCLUSIONES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"REPORTE DE VALIDACIÓN - IMPLEMENTACIÓN PCA DESDE CERO\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"DATOS\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Dimensiones originales:  {X.shape[0]} muestras x {X.shape[1]} características\")\n",
        "print(f\"Dimensiones reducidas:   {X_pca_custom.shape[0]} muestras x {X_pca_custom.shape[1]} componentes\")\n",
        "print(f\"Reducción dimensional:   {(1 - X_pca_custom.shape[1]/X.shape[1])*100:.1f}%\")\n",
        "print(f\"Número de clases:        {len(np.unique(y))}\")\n",
        "\n",
        "print(\"\\nVARIANZA EXPLICADA\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Implementación propia:\")\n",
        "for i, var in enumerate(explained_var_custom[:5]):\n",
        "    print(f\"  PC{i+1}: {var*100:.2f}%\")\n",
        "print(f\"  Total: {np.sum(explained_var_custom)*100:.2f}%\")\n",
        "\n",
        "print(f\"\\nsklearn:\")\n",
        "for i, var in enumerate(explained_var_sklearn):\n",
        "    print(f\"  PC{i+1}: {var*100:.2f}%\")\n",
        "print(f\"  Total: {np.sum(explained_var_sklearn)*100:.2f}%\")\n",
        "\n",
        "print(\"\\nVALIDACIÓN\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Correlación entre implementaciones:\")\n",
        "for i, corr in enumerate(correlations):\n",
        "    print(f\"  PC{i+1}: {corr:.6f}\")\n",
        "\n",
        "print(f\"\\nMétricas de error:\")\n",
        "print(f\"  - MSE:           {mse:.6e}\")\n",
        "print(f\"  - MAE:           {mae:.6e}\")\n",
        "print(f\"  - Max diff:      {max_diff:.6e}\")\n",
        "print(f\"  - Mean diff:     {mean_diff:.6e}\")\n",
        "\n",
        "print(f\"\\nInterpretación:\")\n",
        "print(f\"  - Correlación > 0.99:  Excelente concordancia\")\n",
        "print(f\"  - Correlación > 0.95:  Buena concordancia\")\n",
        "print(f\"  - Correlación < 0.95:  Revisar implementación\")\n",
        "\n",
        "print(f\"\\nCONCLUSIÓN\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Estado de la implementación: {status}\")\n",
        "print(f\"Correlación promedio: {avg_corr:.6f}\")\n",
        "\n",
        "if avg_corr > 0.99:\n",
        "    print(f\"\\n✓ La implementación produce resultados prácticamente idénticos a sklearn.\")\n",
        "    print(f\"✓ El algoritmo PCA implementado desde cero es correcto y eficiente.\")\n",
        "elif avg_corr > 0.95:\n",
        "    print(f\"\\n✓ La implementación produce resultados muy similares a sklearn.\")\n",
        "    print(f\"✓ Pequeñas diferencias pueden deberse a precisiones numéricas.\")\n",
        "else:\n",
        "    print(f\"\\n⚠ La implementación requiere revisión para mejorar la precisión.\")\n",
        "\n",
        "print(f\"\\nCARACTERÍSTICAS DE LA IMPLEMENTACIÓN\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"✓ Algoritmo Power Iteration para eigenvalores\")\n",
        "print(f\"✓ Ordenamiento por eigenvalues descendente\")\n",
        "print(f\"✓ Centrado de datos automático\")\n",
        "print(f\"✓ Cálculo de matriz de covarianza\")\n",
        "print(f\"✓ Proyección a componentes principales\")\n",
        "print(f\"✓ Compatible con diferentes tipos de datos\")\n",
        "print(f\"✓ Validación exhaustiva contra sklearn\")\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 70)\n",
        "print(\"ANÁLISIS COMPLETO FINALIZADO EXITOSAMENTE\")\n",
        "print(\"=\" * 70)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
